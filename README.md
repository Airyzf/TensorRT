# TensorRT
This repository contains docker image of ubuntu16.04+cuda9.0+cudnn7.0+tensorrt3.0
t
The following sentences are quited from https://devblogs.nvidia.com/parallelforall/int8-inference-autonomous-vehicles-tensorrt/, which is developed by Joohoon Lee.


TensorRT is a high-performance deep learning inference optimizer and runtime engine for production deployment of deep learning applications. Developers can optimize models trained in TensorFlow or Caffe to generate memory-efficient runtime engines that maximize inference throughput, making deep learning practical for latency-critical products and services like autonomous driving..

The latest TensorRT 3 release introduces a fully-featured Python API, which enables researchers and developers to optimize and serialize their DNN using familiar Python code. With TensorRT 3 you can deploy models either in Python, for cloud services, or in C++ for real-time applications such as autonomous driving software running on the NVIDIA DRIVE PX AI car computer.
